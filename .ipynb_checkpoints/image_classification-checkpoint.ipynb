{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.preprocessing import image\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directories where the images for training and testing are\n",
    "train_dir = \"datasets/train\"\n",
    "test_dir = \"datasets/test\"\n",
    "\n",
    "#importing as training set labels\n",
    "list_labels = pd.read_csv('datasets/train.truth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fn</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-10049200_1891-09-16_1958.jpg</td>\n",
       "      <td>rotated_left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-10110600_1985-09-17_2012.jpg</td>\n",
       "      <td>rotated_left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-10126400_1964-07-07_2010.jpg</td>\n",
       "      <td>upright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-1013900_1917-10-15_1960.jpg</td>\n",
       "      <td>rotated_right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0-10166400_1960-03-12_2008.jpg</td>\n",
       "      <td>upside_down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0-102100_1970-10-09_2008.jpg</td>\n",
       "      <td>rotated_left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0-1024100_1982-06-07_2011.jpg</td>\n",
       "      <td>rotated_left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0-10292500_1984-03-26_2009.jpg</td>\n",
       "      <td>rotated_left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0-1035700_1945-11-27_2013.jpg</td>\n",
       "      <td>upside_down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0-10416800_1907-01-08_1967.jpg</td>\n",
       "      <td>upright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0-10525500_1916-02-14_1951.jpg</td>\n",
       "      <td>upside_down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0-1054800_1947-09-19_2011.jpg</td>\n",
       "      <td>rotated_left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0-10623500_1931-09-25_1956.jpg</td>\n",
       "      <td>upside_down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0-10726900_1991-02-03_2010.jpg</td>\n",
       "      <td>rotated_right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0-10870400_1971-06-28_2013.jpg</td>\n",
       "      <td>rotated_left</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                fn          label\n",
       "0   0-10049200_1891-09-16_1958.jpg   rotated_left\n",
       "1   0-10110600_1985-09-17_2012.jpg   rotated_left\n",
       "2   0-10126400_1964-07-07_2010.jpg        upright\n",
       "3    0-1013900_1917-10-15_1960.jpg  rotated_right\n",
       "4   0-10166400_1960-03-12_2008.jpg    upside_down\n",
       "5     0-102100_1970-10-09_2008.jpg   rotated_left\n",
       "6    0-1024100_1982-06-07_2011.jpg   rotated_left\n",
       "7   0-10292500_1984-03-26_2009.jpg   rotated_left\n",
       "8    0-1035700_1945-11-27_2013.jpg    upside_down\n",
       "9   0-10416800_1907-01-08_1967.jpg        upright\n",
       "10  0-10525500_1916-02-14_1951.jpg    upside_down\n",
       "11   0-1054800_1947-09-19_2011.jpg   rotated_left\n",
       "12  0-10623500_1931-09-25_1956.jpg    upside_down\n",
       "13  0-10726900_1991-02-03_2010.jpg  rotated_right\n",
       "14  0-10870400_1971-06-28_2013.jpg   rotated_left"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_labels.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples of training sets:  4\n",
      "Number of samples of test sets:  5362\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples of training sets: \", len(os.listdir(train_dir)))\n",
    "print(\"Number of samples of test sets: \", len(os.listdir(test_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    As the images have the same dimension, checking the dimension of the first image in the directory\n",
    "    to load all images through ImageDataGenerator with the correct target_size.\n",
    "\"\"\"\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open(test_dir+'/90-10184590_1979-06-16_2006.jpg')\n",
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data structure already exists for the network to be created!!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The data_manipulations_validate function was created so that the images of the training set are grouped according to \n",
    "    their label, present in the file train.truth.csv.\n",
    "    So the function separates the images between the labels: rotated_right, rotated_left, upright, upside_down.\n",
    "    So that the imagedatagenerator method can be used.\n",
    "\"\"\"\n",
    "def data_manipulations_validate():\n",
    "    dir_mk = list_labels[\"label\"].value_counts().keys().tolist()\n",
    "    value=[1 if os.path.exists(train_dir+'/'+d) == True else 0 for d in dir_mk]\n",
    "    if value != [1,1,1,1]:\n",
    "        print('Creating a structure of data for training the network...')\n",
    "        for d in dir_mk:\n",
    "            os.makedirs(train_dir+'/'+d)\n",
    "        \n",
    "        for index, row in list_labels.iterrows():\n",
    "            shutil.move(train_dir+'/'+row[\"fn\"], train_dir+'/'+row[\"label\"])\n",
    "        print(\"Structure create!\")\n",
    "    else:\n",
    "        print(\"The data structure already exists for the network to be created!!\")\n",
    "data_manipulations_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregamento das imagens utilizando o imagedatagenerator, que gera um lote de dados de imagem(tensor) pra serem utilizados no treinamento da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Creating the ImageDataGenerator with the normalization of the values ​​that will be assigned.\n",
    "\"\"\"\n",
    "\n",
    "data_gen = ImageDataGenerator(rescale=1.0/255.0, zoom_range = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48896 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Using the ImageDataGenerator function, we will already make the dataset be\n",
    "    Loaded and resized in 64x64 image arrays therefore, it is not necessary to resize and normalize them after loading the data.\n",
    "    The batch value will be kept the default which is 32, and the class_mode in this case will be used as a category.\n",
    "    Since the images will be kept at their original RGB values, that is, they will not be changed to grayscale,\n",
    "    the color_mode flag was then passed as rgb.\n",
    "\"\"\"\n",
    "batch_s = 32\n",
    "train_data = data_gen.flow_from_directory(directory = train_dir, \n",
    "                                          target_size =img.size, \n",
    "                                          batch_size = batch_s,\n",
    "                                          color_mode = \"rgb\",\n",
    "                                          class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data:  (64, 64, 3)\n",
      "Classes - Train_data:  [0 0 0 ... 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "print('Train_data: ',train_data.image_shape)\n",
    "print('Classes - Train_data: ',train_data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating network structure\n",
    "\n",
    "\n",
    "Assembling the structure of the CNN model that will be used for the classification of the dataset images.\n",
    "In addition to the input, output and hidden layers, the dropout technique will be used, which is a regularization technique\n",
    "to avoid over-adjustment of the model in training and thus end up skewing the results of the model in question.\n",
    "\n",
    "The dropout technique is to invalidate the use of certain neurons during the training of the network, making the neurons more independent of the weights acquired from their neighbors, thus improving the generalization of receipts as a whole.\n",
    "Three convolutional layers are used.\n",
    "We also use three layers of polling so that we can sub-sample (decrease) the input images, thus reducing the computational load of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    *Parâmetros gerais da estrutura do modelo:\n",
    "    - Kernel = 3 => (3x3).\n",
    "    - Função de ativação será a relu.\n",
    "    - Dimensão de entrada será igual a dimensão das imagens do dataset, sendo composto de 64x64 com dimensão 3,\n",
    "    que faz referência ao valor do RGB para o pixel em específico.\n",
    "    - O valor de variância dos neurônios desativados se iniciará em uma taxa de 0.3 e decrescerá até 0.1.\n",
    "    Uma camada de entrada, 2 camadas de conversão2d, 2 camadas ocultas e uma camada de saída da rede.\n",
    "    \n",
    "    \n",
    "    * General parameters of the model structure:\n",
    "    - Kernel = 3 => (3x3).\n",
    "    - Activation function will be relu.\n",
    "    - Input dimension will be equal to the dimension of the dataset images, being composed of 64x64 with dimension 3,\n",
    "    which refers to the RGB value for the specific pixel.\n",
    "    - The variance value of the deactivated neurons will start at a rate of 0.3 and decrease to 0.1.\n",
    "    One input layer, 2 layers of 2D conversion, 2 hidden layers and one network output layer.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "mdl = Sequential()\n",
    "mdl.add(Conv2D(32, kernel_size=3, activation='relu', input_shape = train_data.image_shape))\n",
    "mdl.add(MaxPooling2D(pool_size=(2,2)))\n",
    "mdl.add(Dropout(rate=0.3))\n",
    "mdl.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "mdl.add(MaxPooling2D(pool_size=(2,2)))\n",
    "mdl.add(Dropout(rate=0.2))\n",
    "mdl.add(Conv2D(126, kernel_size=3, activation='relu'))\n",
    "mdl.add(MaxPooling2D(pool_size=(2,2)))\n",
    "mdl.add(Dropout(rate=0.15))\n",
    "#flattening layer\n",
    "mdl.add(Flatten())\n",
    "mdl.add(Dense(32, activation='relu'))\n",
    "mdl.add(Dropout(rate=0.15))\n",
    "mdl.add(Dense(64, activation='relu'))\n",
    "mdl.add(Dropout(rate=0.1))\n",
    "#network output layer\n",
    "mdl.add(Dense(len(set(train_data.classes)), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    * Em primeiro momento teremos a seguinte configuração de compilação do modelo:\n",
    "    - Será utilizado adam como métrica para a taxa de apredizado da rede.\n",
    "    - Para a função de perda será utilizado a categorical_crossentropy.\n",
    "    - Utilizando accuracy para mensurar a precisão(acurácia) da rede.\n",
    "    \n",
    "    \n",
    "    * Firstly, we will have the following model build configuration:\n",
    "    - Adam will be used as a metric for the network learning rate.\n",
    "    - For the loss function, categorical_crossentropy will be used.\n",
    "    - Using accuracy to measure the precision (accuracy) of the network.\n",
    "\"\"\"\n",
    "\n",
    "mdl.compile(optimizer = \"adam\", loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 29, 29, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 126)       72702     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 126)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 126)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4536)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                145184    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 239,650\n",
      "Trainable params: 239,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN\n",
    "Training the cnn using only 10 epochs and then a graph is plotted with the accuracy of the model obtained after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1528/1528 [==============================] - 139s 91ms/step - loss: 0.5886 - accuracy: 0.7424\n",
      "Epoch 2/10\n",
      "1528/1528 [==============================] - 124s 81ms/step - loss: 0.2252 - accuracy: 0.9126\n",
      "Epoch 3/10\n",
      "1528/1528 [==============================] - 139s 91ms/step - loss: 0.1552 - accuracy: 0.9423\n",
      "Epoch 4/10\n",
      "1528/1528 [==============================] - 140s 91ms/step - loss: 0.1192 - accuracy: 0.9569\n",
      "Epoch 5/10\n",
      "1528/1528 [==============================] - 133s 87ms/step - loss: 0.1018 - accuracy: 0.9617\n",
      "Epoch 6/10\n",
      "1528/1528 [==============================] - 135s 88ms/step - loss: 0.0904 - accuracy: 0.9674\n",
      "Epoch 7/10\n",
      "1528/1528 [==============================] - 141s 92ms/step - loss: 0.0850 - accuracy: 0.9688\n",
      "Epoch 8/10\n",
      "1528/1528 [==============================] - 141s 92ms/step - loss: 0.0779 - accuracy: 0.9708\n",
      "Epoch 9/10\n",
      "1528/1528 [==============================] - 142s 93ms/step - loss: 0.0756 - accuracy: 0.9728\n",
      "Epoch 10/10\n",
      "1528/1528 [==============================] - 148s 97ms/step - loss: 0.0715 - accuracy: 0.9738\n"
     ]
    }
   ],
   "source": [
    "fit_mdl = mdl.fit_generator(train_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mdl = fit_mdl.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb3f414b4d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(acc_mdl)), acc_mdl, 'bo', label = 'accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the accuracy value already shown in the graph, it appears that the model with the current configuration managed to converge well to the above values.\n",
    "Despite the use of dropout on the network, overfitting may have occurred with the training data used.\n",
    "In general, the prediction of the network obtained an accuracy of 96 ~ 97% for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    function that receives the image and returns the image classification by the trained model\n",
    "\"\"\"\n",
    "\n",
    "def tst_img(test):\n",
    "    test_img = image.load_img(test, target_size=(64, 64))\n",
    "    test_img = image.img_to_array(test_img)\n",
    "    test_img = np.expand_dims(test_img, axis = 0)\n",
    "    result = mdl.predict(x = test_img)\n",
    "    predict = ''\n",
    "    #print(result)\n",
    "    if result[0][3] == 1 or max(result[0]) == result[0][3]:\n",
    "        predict = 'upside_down'\n",
    "    if result[0][2] == 1 or max(result[0]) == result[0][2]:\n",
    "        predict = 'upright'\n",
    "    if result[0][1] == 1 or max(result[0]) == result[0][1]:\n",
    "        predict = 'rotated_right'\n",
    "    if result[0][0] == 1 or max(result[0]) == result[0][0]:\n",
    "        predict = 'rotated_left'\n",
    "    return predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res_list=[]\n",
    "#[res_list.append(tst_img(jpgs_test[index])) for index in range(len(jpgs_test))]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Function for saving the names of the test images and classifying them using the tst_img function.\n",
    "    The result is inserted in a list, along with the name of the image that will later be transformed into a .csv file\n",
    "\"\"\"\n",
    "\n",
    "def create_csv_to_result():\n",
    "    way_test = [os.path.join(test_dir, nome) for nome in os.listdir(test_dir)]\n",
    "    arc_test = [arc for arc in way_test if os.path.isfile(arc)]\n",
    "    jpgs_test = [arc for arc in arc_test if arc.lower().endswith(\".jpg\")]\n",
    "\n",
    "    res_list=[[jpgs_test[index][len(test_dir)+1:],tst_img(jpgs_test[index])] for index in range(len(jpgs_test))]\n",
    "    \n",
    "    df_pred=pd.DataFrame(res_list,columns=['fn', 'label'])\n",
    "    df_pred.to_csv('test.preds.csv', index=False)\n",
    "    \n",
    "    print('File with the result of the image classification created.')\n",
    "\n",
    "create_csv_to_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model in disk.\n"
     ]
    }
   ],
   "source": [
    "def save_model():\n",
    "    mdl_json = mdl.to_json()\n",
    "    with open(\"trained_model.json\", \"w\") as json_file:\n",
    "        json_file.write(mdl_json)\n",
    "\n",
    "    mdl.save_weights(\"model.h5\")\n",
    "    print(\"Saved model in disk.\")\n",
    "    \n",
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagens corrigidas e output.zip com as imagens corrigidas!\n"
     ]
    }
   ],
   "source": [
    "#Correcting the orientations of the images that were used for testing.\n",
    "def correction_orientation():\n",
    "    #creating the corrected image directory.\n",
    "    os.makedirs('img_correction_orientation')\n",
    "    #reading the .csv with the name and orientation of test images after sorting\n",
    "    arc_preds = pd.read_csv('test.preds.csv')\n",
    "\n",
    "    for index, row in arc_preds.iterrows():\n",
    "        if row['label'] == 'rotated_left':\n",
    "            im = Image.open(test_dir+'/'+row[\"fn\"]).rotate(-90)\n",
    "            im.save('img_correction_orientation/'+row[\"fn\"])\n",
    "            \n",
    "        if row['label'] == 'rotated_right':\n",
    "            im = Image.open(test_dir+'/'+row[\"fn\"]).rotate(90)\n",
    "            im.save('img_correction_orientation/'+row[\"fn\"])\n",
    "        \n",
    "        if row['label'] == 'upside_down':\n",
    "            im = Image.open(test_dir+'/'+row[\"fn\"]).rotate(180)\n",
    "            im.save('img_correction_orientation/'+row[\"fn\"])\n",
    "\n",
    "        if row['label'] == 'upright':\n",
    "            im = Image.open(test_dir+'/'+row[\"fn\"])\n",
    "            im.save('img_correction_orientation/'+row[\"fn\"])\n",
    "            \n",
    "    #compressing the folder with the corrected images.\n",
    "    shutil.make_archive('output','zip', 'img_correction_orientation')\n",
    "    print('Corrected images and output.zip with corrected images!')\n",
    "correction_orientation()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Matrix ready!\n"
     ]
    }
   ],
   "source": [
    "def numpy_array_img_save():\n",
    "    dir_saved_corrected_imgs='img_correction_orientation'\n",
    "    #reading the .csv with the name and orientation of test images after sorting\n",
    "    arc_preds = pd.read_csv('test.preds.csv')\n",
    "    #Using the pickle dump method to create the numpy image array\n",
    "    arr_imgs = [np.array(Image.open(dir_saved_corrected_imgs+'/'+row['fn'])) for index, row in arc_preds.iterrows()]\n",
    "    pickle.dump(arr_imgs, open('matrix_img.npy','wb'), protocol=2)\n",
    "    #arr_imgs.dump('matrix_img.npy')\n",
    "    print('Numpy Matrix ready!')\n",
    "    \n",
    "numpy_array_img_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5361, 64, 64, 3)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.array(np.load('matrix_img.npy', allow_pickle=True))\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
